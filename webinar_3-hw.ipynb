{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wqsdRrTIHyc"
   },
   "source": [
    "Исчерпывающую информацию с теорией, кодом и примерами можно найти в [статье](https://www.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "gdbpCE-gIHyi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Для работы с матрицами\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Матричная факторизация\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight, tfidf_weight # взвешивание\n",
    "\n",
    "# Функции из 1-ого вебинара\n",
    "import os, sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from metrics import precision_at_k, recall_at_k\n",
    "import itertools\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "2IhaU8JDIHyi",
    "outputId": "a3a22636-04a2-4ad4-9774-e0c5cbcd6a09"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>basket_id</th>\n",
       "      <th>day</th>\n",
       "      <th>item_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>sales_value</th>\n",
       "      <th>store_id</th>\n",
       "      <th>retail_disc</th>\n",
       "      <th>trans_time</th>\n",
       "      <th>week_no</th>\n",
       "      <th>coupon_disc</th>\n",
       "      <th>coupon_match_disc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851472</td>\n",
       "      <td>1</td>\n",
       "      <td>1004906</td>\n",
       "      <td>1</td>\n",
       "      <td>1.39</td>\n",
       "      <td>364</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>1631</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851472</td>\n",
       "      <td>1</td>\n",
       "      <td>1033142</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>364</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1631</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851472</td>\n",
       "      <td>1</td>\n",
       "      <td>1036325</td>\n",
       "      <td>1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>364</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>1631</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851472</td>\n",
       "      <td>1</td>\n",
       "      <td>1082185</td>\n",
       "      <td>1</td>\n",
       "      <td>1.21</td>\n",
       "      <td>364</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1631</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851472</td>\n",
       "      <td>1</td>\n",
       "      <td>8160430</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>364</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>1631</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851516</td>\n",
       "      <td>1</td>\n",
       "      <td>826249</td>\n",
       "      <td>2</td>\n",
       "      <td>1.98</td>\n",
       "      <td>364</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>1642</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851516</td>\n",
       "      <td>1</td>\n",
       "      <td>1043142</td>\n",
       "      <td>1</td>\n",
       "      <td>1.57</td>\n",
       "      <td>364</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>1642</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851516</td>\n",
       "      <td>1</td>\n",
       "      <td>1085983</td>\n",
       "      <td>1</td>\n",
       "      <td>2.99</td>\n",
       "      <td>364</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>1642</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851516</td>\n",
       "      <td>1</td>\n",
       "      <td>1102651</td>\n",
       "      <td>1</td>\n",
       "      <td>1.89</td>\n",
       "      <td>364</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1642</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851516</td>\n",
       "      <td>1</td>\n",
       "      <td>6423775</td>\n",
       "      <td>1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>364</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>1642</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id    basket_id  day  item_id  quantity  sales_value  store_id  \\\n",
       "0     2375  26984851472    1  1004906         1         1.39       364   \n",
       "1     2375  26984851472    1  1033142         1         0.82       364   \n",
       "2     2375  26984851472    1  1036325         1         0.99       364   \n",
       "3     2375  26984851472    1  1082185         1         1.21       364   \n",
       "4     2375  26984851472    1  8160430         1         1.50       364   \n",
       "5     2375  26984851516    1   826249         2         1.98       364   \n",
       "6     2375  26984851516    1  1043142         1         1.57       364   \n",
       "7     2375  26984851516    1  1085983         1         2.99       364   \n",
       "8     2375  26984851516    1  1102651         1         1.89       364   \n",
       "9     2375  26984851516    1  6423775         1         2.00       364   \n",
       "\n",
       "   retail_disc  trans_time  week_no  coupon_disc  coupon_match_disc  \n",
       "0        -0.60        1631        1          0.0                0.0  \n",
       "1         0.00        1631        1          0.0                0.0  \n",
       "2        -0.30        1631        1          0.0                0.0  \n",
       "3         0.00        1631        1          0.0                0.0  \n",
       "4        -0.39        1631        1          0.0                0.0  \n",
       "5        -0.60        1642        1          0.0                0.0  \n",
       "6        -0.68        1642        1          0.0                0.0  \n",
       "7        -0.40        1642        1          0.0                0.0  \n",
       "8         0.00        1642        1          0.0                0.0  \n",
       "9        -0.79        1642        1          0.0                0.0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../webinar_2/webinar_2/data/transaction_data.csv')\n",
    "\n",
    "data.columns = [col.lower() for col in data.columns]\n",
    "data.rename(columns={'household_key': 'user_id',\n",
    "                    'product_id': 'item_id'},\n",
    "           inplace=True)\n",
    "\n",
    "\n",
    "test_size_weeks = 3\n",
    "\n",
    "data_train = data[data['week_no'] < data['week_no'].max() - test_size_weeks]\n",
    "data_test = data[data['week_no'] >= data['week_no'].max() - test_size_weeks]\n",
    "\n",
    "data_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[825123, 831447, 840361, 845307, 852014, 85498...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[854852, 930118, 1077555, 1098066, 5567388, 55...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                             actual\n",
       "0        1  [825123, 831447, 840361, 845307, 852014, 85498...\n",
       "1        2  [854852, 930118, 1077555, 1098066, 5567388, 55..."
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "result_train = data_train.groupby('user_id')['item_id'].unique().reset_index()\n",
    "result_train.columns=['user_id', 'actual']\n",
    "result_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "vBJcGBpTIHyl",
    "outputId": "612dfe1f-286b-4d81-af92-721c6e317b6c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[879517, 934369, 1115576, 1124029, 5572301, 65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[823704, 834117, 840244, 913785, 917816, 93870...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                             actual\n",
       "0        1  [879517, 934369, 1115576, 1124029, 5572301, 65...\n",
       "1        3  [823704, 834117, 840244, 913785, 917816, 93870..."
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "result = data_test.groupby('user_id')['item_id'].unique().reset_index()\n",
    "result.columns=['user_id', 'actual']\n",
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "9dmp1II9IHyl"
   },
   "outputs": [],
   "source": [
    "#train\n",
    "popularity = data_train.groupby('item_id')['quantity'].sum().reset_index()\n",
    "popularity.rename(columns={'quantity': 'n_sold'}, inplace=True)\n",
    "\n",
    "top_5000 = popularity.sort_values('n_sold', ascending=False).head(5000).item_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "jvQ8_CJ9IHym",
    "outputId": "ebf07e9a-6f11-4aa9-d43d-98c0055ec42d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>item_id</th>\n",
       "      <th>202291</th>\n",
       "      <th>397896</th>\n",
       "      <th>420647</th>\n",
       "      <th>480014</th>\n",
       "      <th>545926</th>\n",
       "      <th>707683</th>\n",
       "      <th>731106</th>\n",
       "      <th>818980</th>\n",
       "      <th>819063</th>\n",
       "      <th>819227</th>\n",
       "      <th>...</th>\n",
       "      <th>15926885</th>\n",
       "      <th>15926886</th>\n",
       "      <th>15926887</th>\n",
       "      <th>15926927</th>\n",
       "      <th>15927033</th>\n",
       "      <th>15927403</th>\n",
       "      <th>15927661</th>\n",
       "      <th>15927850</th>\n",
       "      <th>16809471</th>\n",
       "      <th>17105257</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 5001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "item_id  202291    397896    420647    480014    545926    707683    731106    \\\n",
       "user_id                                                                         \n",
       "1             0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2             0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3             0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "item_id  818980    819063    819227    ...  15926885  15926886  15926887  \\\n",
       "user_id                                ...                                 \n",
       "1             0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "2             0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "3             0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "\n",
       "item_id  15926927  15927033  15927403  15927661  15927850  16809471  17105257  \n",
       "user_id                                                                        \n",
       "1             2.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "2             1.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "3             0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[3 rows x 5001 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "# Заведем фиктивный item_id\n",
    "\n",
    "data_train.loc[~data_train['item_id'].isin(top_5000), 'item_id'] = 999999\n",
    "\n",
    "user_item_matrix = pd.pivot_table(data_train, \n",
    "                                  index='user_id', columns='item_id', \n",
    "                                  values='quantity', # Можно пробовать другие варианты\n",
    "                                  aggfunc='count', \n",
    "                                  fill_value=0\n",
    "                                 )\n",
    "\n",
    "user_item_matrix = user_item_matrix.astype(float) # необходимый тип матрицы для implicit\n",
    "\n",
    "# переведем в формат sparse matrix\n",
    "sparse_user_item = csr_matrix(user_item_matrix).tocsr()\n",
    "\n",
    "user_item_matrix.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "TiPuXLb7IHym"
   },
   "outputs": [],
   "source": [
    "#train\n",
    "userids = user_item_matrix.index.values\n",
    "itemids = user_item_matrix.columns.values\n",
    "\n",
    "matrix_userids = np.arange(len(userids))\n",
    "matrix_itemids = np.arange(len(itemids))\n",
    "\n",
    "id_to_itemid = dict(zip(matrix_itemids, itemids))\n",
    "id_to_userid = dict(zip(matrix_userids, userids))\n",
    "\n",
    "itemid_to_id = dict(zip(itemids, matrix_itemids))\n",
    "userid_to_id = dict(zip(userids, matrix_userids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f86969ff8f854a2cb5128440a2a2734e"
     ]
    },
    "id": "3DlT5ppEIHyn",
    "outputId": "c5207262-0dfc-4e42-91bc-e9754aad1906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.1 s, sys: 1.96 s, total: 18.1 s\n",
      "Wall time: 7.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#train\n",
    "model = AlternatingLeastSquares(factors=100, #количество k или размерность который мы подбираем для матриц при разложении , \n",
    "                                # лучший параметр при разложении по версии статьи из вебинара 300-400\n",
    "                                regularization=0.001,# можно подобрать лучший\n",
    "                                iterations=15, # можно подобрать лучший\n",
    "                                calculate_training_loss=True, \n",
    "                                num_threads=1) # 4 потока\n",
    "\n",
    "model.fit(csr_matrix(user_item_matrix).T.tocsr(),  # На вход item-user matrix\n",
    "          show_progress=False)\n",
    "\n",
    "recs = model.recommend(userid=userid_to_id[6],  # userid - id от 0 до N\n",
    "                        user_items=csr_matrix(user_item_matrix).tocsr(),   # на вход user-item matrix\n",
    "                        N=5, # кол-во рекомендаций \n",
    "                        filter_already_liked_items=False, \n",
    "                        filter_items=None, \n",
    "                        recalculate_user=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "NJR0pKy4IHyn",
    "outputId": "f8ea5a18-2e98-43fe-9ec4-5a6f530bf258"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[923746, 1023720, 1051516, 1007195, 866211]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id_to_itemid[rec[0]] for rec in recs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "m2MhJPAcIHyo"
   },
   "outputs": [],
   "source": [
    "def get_recommendations(user, model, N=5):\n",
    "    res = [id_to_itemid[rec[0]] for rec in \n",
    "                    model.recommend(userid=userid_to_id[user], \n",
    "                                    user_items=sparse_user_item,   # на вход user-item matrix\n",
    "                                    N=N, \n",
    "                                    filter_already_liked_items=False, \n",
    "                                    filter_items=None, \n",
    "                                    recalculate_user=True)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "-zbMDYN4IHyo",
    "outputId": "a042ce17-abe1-4b75-aedb-53a3f5c8d839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43 s, sys: 3.95 s, total: 46.9 s\n",
      "Wall time: 23.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14876946258161727"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#test    \n",
    "result['als'] = result['user_id'].apply(lambda x: get_recommendations(x, model=model, N=5))\n",
    "result.apply(lambda row: precision_at_k(row['als'], row['actual']), axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "9TqsTlZEIHyp",
    "outputId": "e040c9f0-75ff-4b9d-ab7b-329e30dc5e09"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>actual</th>\n",
       "      <th>als</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[879517, 934369, 1115576, 1124029, 5572301, 65...</td>\n",
       "      <td>[1029743, 979707, 995242, 962568, 5569374]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[823704, 834117, 840244, 913785, 917816, 93870...</td>\n",
       "      <td>[1133018, 1106523, 5568378, 5569327, 938700]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                             actual  \\\n",
       "0        1  [879517, 934369, 1115576, 1124029, 5572301, 65...   \n",
       "1        3  [823704, 834117, 840244, 913785, 917816, 93870...   \n",
       "\n",
       "                                            als  \n",
       "0    [1029743, 979707, 995242, 962568, 5569374]  \n",
       "1  [1133018, 1106523, 5568378, 5569327, 938700]  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.7 s, sys: 4.38 s, total: 52.1 s\n",
      "Wall time: 26.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.73664"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#train   \n",
    "result_train['als'] = result_train['user_id'].apply(lambda x: get_recommendations(x, model=model, N=5))\n",
    "result_train.apply(lambda row: precision_at_k(row['als'], row['actual']), axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>actual</th>\n",
       "      <th>als</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[825123, 831447, 840361, 845307, 852014, 85498...</td>\n",
       "      <td>[1029743, 979707, 995242, 962568, 5569374]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[854852, 930118, 1077555, 1098066, 5567388, 55...</td>\n",
       "      <td>[1133018, 1106523, 999999, 5569230, 1082185]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                             actual  \\\n",
       "0        1  [825123, 831447, 840361, 845307, 852014, 85498...   \n",
       "1        2  [854852, 930118, 1077555, 1098066, 5567388, 55...   \n",
       "\n",
       "                                            als  \n",
       "0    [1029743, 979707, 995242, 962568, 5569374]  \n",
       "1  [1133018, 1106523, 999999, 5569230, 1082185]  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(row, header=False, spacing=12):\n",
    "    top = ''\n",
    "    middle = ''\n",
    "    bottom = ''\n",
    "    for r in row:\n",
    "        top += '+{}'.format('-'*spacing)\n",
    "        if isinstance(r, str):\n",
    "            middle += '| {0:^{1}} '.format(r, spacing-2)\n",
    "        elif isinstance(r, int):\n",
    "            middle += '| {0:^{1}} '.format(r, spacing-2)\n",
    "        elif isinstance(r, float):\n",
    "            middle += '| {0:^{1}.5f} '.format(r, spacing-2)\n",
    "        bottom += '+{}'.format('='*spacing)\n",
    "    top += '+'\n",
    "    middle += '|'\n",
    "    bottom += '+'\n",
    "    if header:\n",
    "        print(top)\n",
    "        print(middle)\n",
    "        print(bottom)\n",
    "    else:\n",
    "        print(middle)\n",
    "        print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = csr_matrix(user_item_matrix).T.tocsr()\n",
    "#train_actual = result_train\n",
    "#test_actual = result\n",
    "#epochs=range(2, 40, 2)\n",
    "def learning_curve(model, train, train_actual, test_actual, epochs, k=5): \n",
    "    \n",
    "    prev_epoch = 0\n",
    "    train_precision = []\n",
    "    test_precision = []\n",
    "    \n",
    "    headers = ['epochs', 'p@k train', 'p@k test']\n",
    "    print_log(headers, header=True)\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        model.iterations = epoch - prev_epoch\n",
    "        if not hasattr(model, 'user_vectors'):\n",
    "            model.fit(train, show_progress=False)\n",
    "        else:\n",
    "            model.fit_partial(train, show_progress=False)\n",
    "            \n",
    "        test_actual['als'] = test_actual['user_id'].apply(lambda x: get_recommendations(x, model=model, N=5))\n",
    "        train_actual['als'] = train_actual['user_id'].apply(lambda x: get_recommendations(x, model=model, N=5))\n",
    "        \n",
    "            \n",
    "        train_precision.append(train_actual.apply(lambda row: precision_at_k(row['als'], row['actual']), axis=1).mean())\n",
    "        test_precision.append(test_actual.apply(lambda row: precision_at_k(row['als'], row['actual']), axis=1).mean())\n",
    "        row = [epoch, train_precision[-1], test_precision[-1]]\n",
    "        print_log(row)\n",
    "        prev_epoch = epoch\n",
    "    return model, train_precision, test_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlternatingLeastSquares(factors=100, #количество k или размерность который мы подбираем для матриц при разложении , \n",
    "                                # лучший параметр при разложении по версии статьи из вебинара 300-400\n",
    "                                regularization=0.001,# можно подобрать лучший\n",
    "                                iterations=15, # можно подобрать лучший\n",
    "                                calculate_training_loss=True, \n",
    "                                num_threads=1) # 4 потока\n",
    "model1 , train_precision1, test_precision1 = learning_curve(model, csr_matrix(user_item_matrix).T.tocsr(),\n",
    "                            result_train, result, epochs=range(2, 40, 2), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_learning_curve(base_model, train, train_actual, test_actual, param_grid,\n",
    "                               user_index=None, patk=5, epochs=range(2, 10, 2)):\n",
    "    \"\"\"\n",
    "    \"Inspired\" (stolen) from sklearn gridsearch\n",
    "    https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py\n",
    "    \"\"\"\n",
    "    curves = []\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    for v in itertools.product(*values):\n",
    "        params = dict(zip(keys, v))\n",
    "        this_model = copy.deepcopy(base_model)\n",
    "        print_line = []\n",
    "        for k, v in params.items():\n",
    "            setattr(this_model, k, v)\n",
    "            print_line.append((k, v))\n",
    "\n",
    "        print(' | '.join('{}: {}'.format(k, v) for (k, v) in print_line))\n",
    "        _, train_patk, test_patk = learning_curve(this_model, train, train_actual, test_actual, epochs, k=patk)\n",
    "        curves.append({'params': params,\n",
    "                       'patk': {'train': train_patk, 'test': test_patk}})\n",
    "    return curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'num_factors': [10, 20, 40, 80, 120],\n",
    "              'regularization': [0.0, 1e-5, 1e-3, 1e-1, 1e1, 1e2],\n",
    "              'alpha': [1, 10, 50, 100, 500, 1000]}\n",
    "base_model = AlternatingLeastSquares()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_factors: 10 | regularization: 0.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66088   |  0.17991   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69192   |  0.16223   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71024   |  0.15460   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71992   |  0.15088   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66184   |  0.17649   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69440   |  0.16123   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70824   |  0.15530   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71752   |  0.15078   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66400   |  0.17569   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69392   |  0.15711   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70968   |  0.15028   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71664   |  0.14495   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66560   |  0.17850   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69448   |  0.16313   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71072   |  0.15721   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72016   |  0.15319   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66312   |  0.17800   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69560   |  0.16344   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71216   |  0.15389   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72168   |  0.15098   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66512   |  0.17941   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69640   |  0.16092   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71504   |  0.15821   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72408   |  0.15349   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 1e-05 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66416   |  0.17549   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69736   |  0.16042   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71424   |  0.15409   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72520   |  0.15148   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 1e-05 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66472   |  0.17951   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69592   |  0.16102   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71360   |  0.15450   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72216   |  0.15359   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 1e-05 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66320   |  0.17780   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69400   |  0.16113   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70544   |  0.15379   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71448   |  0.15118   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 1e-05 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66568   |  0.18001   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69840   |  0.16062   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71336   |  0.15188   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72304   |  0.14817   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 1e-05 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66712   |  0.17539   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69928   |  0.16022   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71408   |  0.15520   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72456   |  0.15208   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 1e-05 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.65936   |  0.17459   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69152   |  0.16404   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70720   |  0.15510   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71544   |  0.15118   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.001 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66312   |  0.17770   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69248   |  0.16514   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70592   |  0.15791   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71440   |  0.15389   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.001 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66680   |  0.18242   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69720   |  0.16394   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70968   |  0.15610   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71848   |  0.15229   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.001 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.65712   |  0.17539   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69040   |  0.15871   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70968   |  0.15218   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71960   |  0.15198   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.001 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66496   |  0.17057   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69392   |  0.15841   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70920   |  0.15439   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71760   |  0.15379   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.001 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     2      |  0.66616   |  0.18182   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69576   |  0.15992   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71272   |  0.15078   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72416   |  0.15118   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.001 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66368   |  0.18051   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69528   |  0.15992   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70880   |  0.15369   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71544   |  0.14877   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.1 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67504   |  0.19327   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68712   |  0.16846   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69792   |  0.16042   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70352   |  0.15389   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.1 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67856   |  0.19166   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68432   |  0.16615   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69472   |  0.15580   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70448   |  0.15309   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.1 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.68032   |  0.18985   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69432   |  0.16986   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70040   |  0.16082   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70592   |  0.15771   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.1 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67688   |  0.19307   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69192   |  0.16956   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69760   |  0.15892   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70224   |  0.15821   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.1 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67600   |  0.18785   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69136   |  0.16605   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69944   |  0.15892   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70472   |  0.15530   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 0.1 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67304   |  0.18714   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68400   |  0.17087   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69608   |  0.15962   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70512   |  0.15811   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 10.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.71608   |  0.23335   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.72648   |  0.19136   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73088   |  0.18292   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73368   |  0.17659   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 10.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.72424   |  0.22953   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.73024   |  0.18604   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73064   |  0.17649   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73424   |  0.17690   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 10.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.71888   |  0.23656   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.72704   |  0.19729   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73016   |  0.18523   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73024   |  0.17891   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 10.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.72368   |  0.22662   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.72808   |  0.19086   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.72968   |  0.18503   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73240   |  0.17690   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 10.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.71008   |  0.22853   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.72264   |  0.19417   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.72592   |  0.18001   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72784   |  0.17368   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 10.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.71944   |  0.23044   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.72984   |  0.19407   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.72968   |  0.17971   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73152   |  0.17539   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 100.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51304   |  0.18684   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52200   |  0.18644   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53440   |  0.19086   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.54080   |  0.19357   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 100.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51008   |  0.18553   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.53280   |  0.19096   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.54632   |  0.19699   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.55200   |  0.19900   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 100.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51344   |  0.18473   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52080   |  0.18453   |\n",
      "+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     6      |  0.53208   |  0.18895   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.53816   |  0.19016   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 100.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51072   |  0.18483   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52408   |  0.18724   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53968   |  0.19417   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.54528   |  0.19598   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 100.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51144   |  0.18493   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52152   |  0.18463   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53424   |  0.18965   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.53984   |  0.19216   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 10 | regularization: 100.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51064   |  0.18684   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.53096   |  0.19166   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.54632   |  0.19648   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.55184   |  0.19729   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66104   |  0.17428   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69672   |  0.15922   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71296   |  0.15108   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72312   |  0.15078   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66432   |  0.17700   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69488   |  0.15681   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71216   |  0.15369   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72128   |  0.15098   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66336   |  0.17720   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69016   |  0.16092   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70624   |  0.15249   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71944   |  0.14977   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66616   |  0.17579   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69464   |  0.16203   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70936   |  0.15349   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72152   |  0.15048   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66240   |  0.18051   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69200   |  0.16193   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70888   |  0.15460   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71872   |  0.14877   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66584   |  0.17991   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69792   |  0.16042   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71256   |  0.15319   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72136   |  0.15299   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 1e-05 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66864   |  0.17750   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69576   |  0.16243   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71072   |  0.15600   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72064   |  0.15239   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 1e-05 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66368   |  0.17710   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69072   |  0.15932   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70584   |  0.15580   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71632   |  0.15369   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 1e-05 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66424   |  0.17469   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69824   |  0.16524   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71400   |  0.15620   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72176   |  0.15460   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 1e-05 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66176   |  0.17901   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69224   |  0.16153   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70760   |  0.15741   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71840   |  0.15269   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 1e-05 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66616   |  0.17921   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69896   |  0.15902   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71472   |  0.15379   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72536   |  0.15218   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 1e-05 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66472   |  0.18172   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69328   |  0.15741   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70672   |  0.15419   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71568   |  0.15088   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.001 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66592   |  0.18001   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69584   |  0.16102   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71232   |  0.15439   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72384   |  0.15399   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.001 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     2      |  0.66760   |  0.17680   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69544   |  0.15841   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71248   |  0.15871   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72112   |  0.15470   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.001 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.65888   |  0.17820   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68616   |  0.15781   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70136   |  0.15259   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71136   |  0.14907   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.001 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66632   |  0.17569   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69376   |  0.16123   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70928   |  0.15560   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71872   |  0.15419   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.001 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66448   |  0.17941   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69368   |  0.15902   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70952   |  0.15148   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71864   |  0.14867   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.001 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66688   |  0.17469   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69976   |  0.15892   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71696   |  0.15580   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72368   |  0.15329   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.1 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66800   |  0.18714   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68544   |  0.16826   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69912   |  0.15761   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70608   |  0.15339   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.1 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67096   |  0.18533   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68392   |  0.16715   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69632   |  0.15922   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70568   |  0.15439   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.1 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66480   |  0.18835   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68624   |  0.16605   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69544   |  0.15851   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70480   |  0.15490   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.1 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67880   |  0.19247   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68992   |  0.16946   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69808   |  0.16484   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70752   |  0.16233   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.1 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67320   |  0.19086   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69024   |  0.17047   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69880   |  0.16173   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70712   |  0.15811   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 0.1 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66752   |  0.18734   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68920   |  0.17499   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69688   |  0.16615   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70520   |  0.15881   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 10.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.71656   |  0.22642   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.72208   |  0.18845   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.72616   |  0.17700   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72992   |  0.17388   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 10.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.72568   |  0.23014   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.73296   |  0.19518   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73608   |  0.18031   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73744   |  0.17870   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 10.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.72592   |  0.23315   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.72896   |  0.19427   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.72816   |  0.18172   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73072   |  0.17800   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 10.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.72856   |  0.23335   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.73152   |  0.19437   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73336   |  0.18172   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73416   |  0.17569   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 10.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.70624   |  0.22853   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.71528   |  0.19136   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.72232   |  0.18332   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72776   |  0.17629   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 10.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.72672   |  0.23546   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.73792   |  0.20462   |\n",
      "+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     6      |  0.73808   |  0.18795   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73744   |  0.18232   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 100.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51088   |  0.18664   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.53152   |  0.19096   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.54424   |  0.19638   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.55168   |  0.19779   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 100.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51168   |  0.18654   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.53408   |  0.19297   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.54976   |  0.19849   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.55304   |  0.19749   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 100.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51040   |  0.18644   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52624   |  0.18835   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.54048   |  0.19437   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.54672   |  0.19458   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 100.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51608   |  0.18453   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52112   |  0.18674   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53088   |  0.18825   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.53584   |  0.18995   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 100.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51072   |  0.18644   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.53464   |  0.19146   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.54632   |  0.19709   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.55240   |  0.19769   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 20 | regularization: 100.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.50952   |  0.18674   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52752   |  0.19016   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.54080   |  0.19488   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.54680   |  0.19658   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.65608   |  0.17810   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68944   |  0.15992   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70872   |  0.15470   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72128   |  0.14917   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66464   |  0.17870   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69640   |  0.16002   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71176   |  0.15349   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72152   |  0.15118   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66496   |  0.17981   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69792   |  0.16102   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71248   |  0.15249   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72136   |  0.15128   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66744   |  0.18011   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69112   |  0.15922   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70792   |  0.15188   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72144   |  0.15048   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66376   |  0.17670   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69448   |  0.16273   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71296   |  0.15530   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72648   |  0.15349   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66472   |  0.17438   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69536   |  0.15851   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71080   |  0.15389   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71896   |  0.14746   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 1e-05 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66560   |  0.17981   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69968   |  0.15781   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71568   |  0.15319   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72488   |  0.14957   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 1e-05 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66336   |  0.17800   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69248   |  0.16313   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70496   |  0.15259   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71472   |  0.14787   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 1e-05 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.65912   |  0.17720   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69224   |  0.16404   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70992   |  0.15500   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72016   |  0.14957   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 1e-05 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66256   |  0.17217   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69608   |  0.16143   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71448   |  0.15630   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72512   |  0.15128   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 1e-05 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     2      |  0.66608   |  0.17891   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69632   |  0.15861   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71384   |  0.15610   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72544   |  0.15409   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 1e-05 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66872   |  0.17880   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69768   |  0.16102   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71296   |  0.15269   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72128   |  0.15138   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.001 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.65960   |  0.17840   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69120   |  0.16283   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70816   |  0.15500   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72192   |  0.15480   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.001 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66304   |  0.17469   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69648   |  0.16183   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71208   |  0.15429   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72144   |  0.15148   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.001 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66272   |  0.18122   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69624   |  0.16123   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71472   |  0.15751   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72624   |  0.15520   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.001 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.65744   |  0.17368   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68864   |  0.16082   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70440   |  0.15540   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71184   |  0.15279   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.001 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66224   |  0.17629   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69160   |  0.15751   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70608   |  0.15580   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71960   |  0.15118   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.001 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66424   |  0.18182   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69616   |  0.16072   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71248   |  0.15711   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72248   |  0.15309   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.1 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67568   |  0.19488   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68712   |  0.17047   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69896   |  0.16263   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70552   |  0.15701   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.1 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67856   |  0.19508   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68504   |  0.17338   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69416   |  0.15942   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70440   |  0.15520   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.1 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67728   |  0.19417   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69176   |  0.17358   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70192   |  0.16615   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70896   |  0.16022   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.1 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67032   |  0.18764   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68560   |  0.17037   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69792   |  0.16163   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70288   |  0.15841   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.1 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.68544   |  0.18955   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.70008   |  0.16695   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70632   |  0.15992   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71336   |  0.15550   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 0.1 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67072   |  0.19307   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68616   |  0.16695   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69704   |  0.15560   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70656   |  0.15128   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 10.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.73112   |  0.23285   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.73888   |  0.19658   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73720   |  0.18754   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73704   |  0.18101   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 10.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.72248   |  0.23446   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.73216   |  0.19548   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73208   |  0.18513   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73312   |  0.17911   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 10.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.72424   |  0.22983   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.72624   |  0.19598   |\n",
      "+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     6      |  0.72800   |  0.18805   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73192   |  0.18222   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 10.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.71896   |  0.22793   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.73080   |  0.19006   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73152   |  0.17780   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73424   |  0.17408   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 10.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.70480   |  0.22341   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.71984   |  0.18825   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.72608   |  0.17559   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73088   |  0.17499   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 10.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.72984   |  0.22973   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.73368   |  0.19960   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73480   |  0.18985   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73696   |  0.18222   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 100.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51152   |  0.18553   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52376   |  0.18584   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53872   |  0.19237   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.54376   |  0.19578   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 100.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51192   |  0.18564   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52056   |  0.18403   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53352   |  0.18995   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.53992   |  0.19216   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 100.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51024   |  0.18584   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.53376   |  0.19176   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.54528   |  0.19658   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.55160   |  0.19789   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 100.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51208   |  0.18463   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.51984   |  0.18443   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53144   |  0.18855   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.53824   |  0.19106   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 100.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51568   |  0.18453   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52056   |  0.18443   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53056   |  0.18855   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.53672   |  0.19046   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 40 | regularization: 100.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51152   |  0.18564   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52272   |  0.18664   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53744   |  0.19206   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.54256   |  0.19407   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66216   |  0.18292   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69288   |  0.15731   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71240   |  0.15299   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72504   |  0.15208   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66272   |  0.17720   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69240   |  0.15992   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70912   |  0.15289   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72024   |  0.14987   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66392   |  0.17760   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69176   |  0.16123   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70840   |  0.15500   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71520   |  0.15038   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66752   |  0.18292   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69816   |  0.16323   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71264   |  0.15289   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72480   |  0.15118   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66096   |  0.17770   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68784   |  0.16042   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70824   |  0.15058   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71768   |  0.15148   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66344   |  0.17720   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69648   |  0.16123   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71152   |  0.15309   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71824   |  0.15168   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 1e-05 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66384   |  0.18152   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69504   |  0.15952   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71144   |  0.15229   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71936   |  0.14957   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 1e-05 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     2      |  0.66648   |  0.17810   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69472   |  0.15992   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71312   |  0.15108   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72312   |  0.15269   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 1e-05 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66560   |  0.17649   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69768   |  0.16082   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71392   |  0.15620   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72192   |  0.15309   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 1e-05 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66648   |  0.18202   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69224   |  0.15902   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70456   |  0.15329   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71400   |  0.14596   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 1e-05 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66488   |  0.17840   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69328   |  0.16123   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71272   |  0.15500   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72264   |  0.15359   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 1e-05 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66864   |  0.17760   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69496   |  0.16062   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71016   |  0.15229   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71904   |  0.14877   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.001 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66752   |  0.17780   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69232   |  0.15942   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70680   |  0.15409   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71640   |  0.15218   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.001 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66448   |  0.17479   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69376   |  0.15962   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71168   |  0.15068   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72064   |  0.14897   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.001 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66704   |  0.17619   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69296   |  0.16042   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71056   |  0.15620   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72224   |  0.15239   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.001 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66696   |  0.18031   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69848   |  0.16494   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71288   |  0.15590   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72088   |  0.15118   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.001 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66392   |  0.17991   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69568   |  0.16072   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71424   |  0.15349   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72448   |  0.15289   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.001 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66128   |  0.17840   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69456   |  0.16404   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71224   |  0.15811   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71904   |  0.15500   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.1 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67480   |  0.19297   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69280   |  0.17067   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70304   |  0.16313   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70880   |  0.15811   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.1 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67400   |  0.19458   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69072   |  0.17278   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70032   |  0.16042   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70520   |  0.15892   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.1 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67704   |  0.18895   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68896   |  0.16615   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70168   |  0.16123   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70984   |  0.15711   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.1 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67016   |  0.19036   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68616   |  0.16896   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69624   |  0.16293   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70416   |  0.15831   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.1 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67912   |  0.19709   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69192   |  0.16976   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70184   |  0.16082   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70752   |  0.15600   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 0.1 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67384   |  0.18925   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69160   |  0.16575   |\n",
      "+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     6      |  0.70424   |  0.15881   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71160   |  0.15510   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 10.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.72096   |  0.22793   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.73088   |  0.18985   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73640   |  0.18493   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73896   |  0.17880   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 10.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.70960   |  0.23184   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.72304   |  0.19387   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.72848   |  0.18232   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73160   |  0.17840   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 10.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.71968   |  0.23204   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.73064   |  0.19166   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73352   |  0.18242   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73752   |  0.17830   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 10.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.72832   |  0.23024   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.73360   |  0.19146   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73616   |  0.18332   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73792   |  0.17800   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 10.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.71576   |  0.22873   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.72784   |  0.19417   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73064   |  0.18312   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73248   |  0.17840   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 10.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.71096   |  0.22341   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.71872   |  0.18975   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.72328   |  0.17961   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72568   |  0.17509   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 100.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51120   |  0.18503   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52080   |  0.18413   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53432   |  0.18895   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.54152   |  0.19347   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 100.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.50968   |  0.18553   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52928   |  0.19116   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.54280   |  0.19528   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.54936   |  0.19648   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 100.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51192   |  0.18644   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.53384   |  0.19016   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.54680   |  0.19618   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.55288   |  0.19739   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 100.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51176   |  0.18413   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52312   |  0.18684   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53448   |  0.19036   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.53984   |  0.19357   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 100.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51104   |  0.18594   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52232   |  0.18584   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53632   |  0.19156   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.54168   |  0.19377   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 80 | regularization: 100.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51072   |  0.18533   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52064   |  0.18564   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53520   |  0.18965   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.54224   |  0.19387   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66056   |  0.17770   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68816   |  0.15932   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70784   |  0.15218   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71848   |  0.15078   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66320   |  0.17459   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69504   |  0.16133   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70904   |  0.15630   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71856   |  0.15068   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66240   |  0.17921   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69072   |  0.16384   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70960   |  0.15721   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72152   |  0.15339   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66408   |  0.17880   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69792   |  0.16374   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71480   |  0.15650   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72368   |  0.15239   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     2      |  0.66184   |  0.17750   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69064   |  0.16384   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70536   |  0.15510   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71496   |  0.15158   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66432   |  0.17428   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69160   |  0.15450   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70752   |  0.15128   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71792   |  0.14776   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 1e-05 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66616   |  0.17941   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69496   |  0.16243   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71112   |  0.15460   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72224   |  0.15309   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 1e-05 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66520   |  0.17629   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69368   |  0.15831   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71256   |  0.15309   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72248   |  0.14897   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 1e-05 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66112   |  0.17298   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69464   |  0.15781   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71096   |  0.15610   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71960   |  0.15279   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 1e-05 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66376   |  0.17308   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69080   |  0.15972   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70720   |  0.15580   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71712   |  0.15158   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 1e-05 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66680   |  0.17529   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69760   |  0.15962   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71328   |  0.15419   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72128   |  0.15168   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 1e-05 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66504   |  0.17840   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69736   |  0.15851   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71176   |  0.15379   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72304   |  0.15439   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.001 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66432   |  0.17629   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69520   |  0.15952   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71176   |  0.15691   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71776   |  0.15249   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.001 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66384   |  0.17820   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69440   |  0.15761   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71152   |  0.14887   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72000   |  0.14535   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.001 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66624   |  0.17770   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69584   |  0.16203   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71240   |  0.15520   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72128   |  0.15460   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.001 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66296   |  0.17549   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69376   |  0.15791   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.70848   |  0.15299   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.71712   |  0.14917   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.001 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66272   |  0.17519   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69016   |  0.15871   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71024   |  0.15560   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72032   |  0.15128   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.001 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66496   |  0.17931   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69512   |  0.16233   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71152   |  0.15681   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72192   |  0.15329   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.1 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.68136   |  0.19257   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69352   |  0.17097   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69936   |  0.15982   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70344   |  0.15560   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.1 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.66976   |  0.19247   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68240   |  0.17127   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69264   |  0.15932   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70216   |  0.15650   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.1 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67992   |  0.18403   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68928   |  0.16725   |\n",
      "+------------+------------+------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     6      |  0.70288   |  0.16183   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70968   |  0.15640   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.1 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67040   |  0.19267   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68576   |  0.17298   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69824   |  0.16645   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70400   |  0.15922   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.1 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67224   |  0.19166   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.68704   |  0.16976   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69464   |  0.16193   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70512   |  0.16052   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 0.1 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.67088   |  0.19066   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.69064   |  0.16554   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.69848   |  0.15379   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.70056   |  0.15178   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 10.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.71552   |  0.22793   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.72664   |  0.19427   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.72712   |  0.18222   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72880   |  0.17710   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 10.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.68952   |  0.22069   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.71352   |  0.18895   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.71936   |  0.17840   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72576   |  0.17700   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 10.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.71856   |  0.22983   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.73104   |  0.19448   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.73168   |  0.18081   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.73376   |  0.17639   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 10.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.69288   |  0.22933   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.71840   |  0.19759   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.72128   |  0.18433   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72328   |  0.17870   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 10.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.74112   |  0.23666   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.74448   |  0.20211   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.74240   |  0.19076   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.74136   |  0.18192   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 10.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.71112   |  0.22973   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.72072   |  0.19186   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.72592   |  0.17911   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.72816   |  0.17318   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 100.0 | alpha: 1\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51184   |  0.18553   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52304   |  0.18584   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53752   |  0.19186   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.54376   |  0.19488   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 100.0 | alpha: 10\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51296   |  0.18453   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52184   |  0.18413   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53336   |  0.18915   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.53936   |  0.19287   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 100.0 | alpha: 50\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51032   |  0.18584   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.53208   |  0.19196   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.54912   |  0.19699   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.55224   |  0.19689   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 100.0 | alpha: 100\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51064   |  0.18644   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.53424   |  0.19237   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.54864   |  0.19769   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.55232   |  0.19759   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 100.0 | alpha: 500\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51184   |  0.18724   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.53472   |  0.19206   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.54776   |  0.19769   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.55424   |  0.19739   |\n",
      "+------------+------------+------------+\n",
      "num_factors: 120 | regularization: 100.0 | alpha: 1000\n",
      "+------------+------------+------------+\n",
      "|   epochs   | p@k train  |  p@k test  |\n",
      "+============+============+============+\n",
      "|     2      |  0.51248   |  0.18624   |\n",
      "+------------+------------+------------+\n",
      "|     4      |  0.52272   |  0.18614   |\n",
      "+------------+------------+------------+\n",
      "|     6      |  0.53472   |  0.19026   |\n",
      "+------------+------------+------------+\n",
      "|     8      |  0.54208   |  0.19407   |\n",
      "+------------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "curves = grid_search_learning_curve(base_model, csr_matrix(user_item_matrix).T.tocsr(),\n",
    "                            result_train, result,\n",
    "                                    param_grid,\n",
    "                                    user_index=None,\n",
    "                                    patk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_curves = sorted(curves, key=lambda x: max(x['patk']['test']), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_factors': 120, 'regularization': 10.0, 'alpha': 500}\n",
      "0.23666499246609746\n",
      "Epoch: 2\n"
     ]
    }
   ],
   "source": [
    "print(best_curves[0]['params'])\n",
    "max_score = max(best_curves[0]['patk']['test'])\n",
    "print(max_score)\n",
    "iterations = range(2, 40, 2)[best_curves[0]['patk']['test'].index(max_score)]\n",
    "print('Epoch: {}'.format(iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': {'num_factors': 120, 'regularization': 10.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.7411199999999999, 0.74448, 0.7424, 0.7413599999999999],\n",
       "   'test': [0.23666499246609746,\n",
       "    0.2021094927172275,\n",
       "    0.19075841285786038,\n",
       "    0.18191863385233553]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 10.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.71888, 0.7270399999999999, 0.73016, 0.73024],\n",
       "   'test': [0.23656454043194375,\n",
       "    0.19728779507785033,\n",
       "    0.18523355097940733,\n",
       "    0.17890507282772478]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 10.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.72672, 0.73792, 0.73808, 0.73744],\n",
       "   'test': [0.23545956805625318,\n",
       "    0.2046207935710698,\n",
       "    0.187945755901557,\n",
       "    0.1823204419889503]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 10.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.7224799999999999, 0.7321599999999999, 0.73208, 0.73312],\n",
       "   'test': [0.23445504771471623,\n",
       "    0.1954796584630839,\n",
       "    0.18513309894525365,\n",
       "    0.17910597689603217]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 10.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.7160799999999999,\n",
       "    0.7264799999999999,\n",
       "    0.73088,\n",
       "    0.7336799999999999],\n",
       "   'test': [0.23335007533902563,\n",
       "    0.19136112506278252,\n",
       "    0.18292315419387245,\n",
       "    0.17659467604218987]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 10.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.72856,\n",
       "    0.7315199999999998,\n",
       "    0.7333599999999999,\n",
       "    0.7341599999999999],\n",
       "   'test': [0.23335007533902563,\n",
       "    0.19437468608739328,\n",
       "    0.18171772978402817,\n",
       "    0.17569060773480663]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 10.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.72592, 0.72896, 0.72816, 0.73072],\n",
       "   'test': [0.23314917127071824,\n",
       "    0.19427423405323963,\n",
       "    0.18171772978402817,\n",
       "    0.17800100452034157]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 10.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.7311199999999999, 0.73888, 0.7372, 0.7370399999999999],\n",
       "   'test': [0.23284781516825717,\n",
       "    0.1965846308387745,\n",
       "    0.18754394776494226,\n",
       "    0.1810145655449523]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 10.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.7196799999999999, 0.73064, 0.73352, 0.7375199999999998],\n",
       "   'test': [0.23204419889502764,\n",
       "    0.19166248116524362,\n",
       "    0.182420894023104,\n",
       "    0.17830236062280264]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 10.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.7096, 0.72304, 0.72848, 0.7316],\n",
       "   'test': [0.23184329482672025,\n",
       "    0.19387242591662482,\n",
       "    0.18232044198895028,\n",
       "    0.17840281265695632]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 10.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.71944, 0.7298399999999999, 0.7296799999999999, 0.73152],\n",
       "   'test': [0.23043696634856856,\n",
       "    0.1940733299849322,\n",
       "    0.17970868910095433,\n",
       "    0.17538925163234556]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 10.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.72832, 0.7336, 0.73616, 0.7379199999999999],\n",
       "   'test': [0.2302360622802612,\n",
       "    0.19146157709693623,\n",
       "    0.1833249623304872,\n",
       "    0.17800100452034154]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 10.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.72568, 0.7329600000000001, 0.7360800000000001, 0.73744],\n",
       "   'test': [0.2301356102461075,\n",
       "    0.1951783023606228,\n",
       "    0.18031140130587645,\n",
       "    0.1787041687594174]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 10.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.72424, 0.72624, 0.728, 0.7319199999999999],\n",
       "   'test': [0.22983425414364642,\n",
       "    0.19598191863385236,\n",
       "    0.18804620793571072,\n",
       "    0.1822199899547966]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 10.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.7185600000000001,\n",
       "    0.7310399999999999,\n",
       "    0.7316799999999999,\n",
       "    0.73376],\n",
       "   'test': [0.22983425414364642,\n",
       "    0.194475138121547,\n",
       "    0.1808136614766449,\n",
       "    0.17639377197388245]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 10.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.7111199999999999, 0.72072, 0.72592, 0.7281599999999998],\n",
       "   'test': [0.22973380210949274,\n",
       "    0.19186338523355098,\n",
       "    0.17910597689603217,\n",
       "    0.17317930688096433]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 10.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.7298399999999999,\n",
       "    0.7336799999999999,\n",
       "    0.7348,\n",
       "    0.7369599999999998],\n",
       "   'test': [0.2297338021094927,\n",
       "    0.19959819186338526,\n",
       "    0.18985434455047714,\n",
       "    0.18221998995479663]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 10.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.72424, 0.73024, 0.73064, 0.73424],\n",
       "   'test': [0.22953289804118535,\n",
       "    0.1860371672526369,\n",
       "    0.17649422400803616,\n",
       "    0.17689603214465094]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 10.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.6928799999999999,\n",
       "    0.7184,\n",
       "    0.7212799999999999,\n",
       "    0.7232799999999999],\n",
       "   'test': [0.22933199397287796,\n",
       "    0.1975891511803114,\n",
       "    0.18432948267202412,\n",
       "    0.17870416875941741]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 10.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.7157600000000001,\n",
       "    0.7278399999999999,\n",
       "    0.73064,\n",
       "    0.7324799999999999],\n",
       "   'test': [0.22872928176795582,\n",
       "    0.1941737820190859,\n",
       "    0.1831240582621798,\n",
       "    0.17840281265695632]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 10.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.7100799999999999,\n",
       "    0.72264,\n",
       "    0.7259200000000001,\n",
       "    0.7278399999999999],\n",
       "   'test': [0.22852837769964843,\n",
       "    0.1941737820190859,\n",
       "    0.18001004520341535,\n",
       "    0.1736815670517328]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 10.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.70624,\n",
       "    0.7152799999999999,\n",
       "    0.7223199999999999,\n",
       "    0.7277600000000001],\n",
       "   'test': [0.22852837769964843,\n",
       "    0.19136112506278252,\n",
       "    0.1833249623304872,\n",
       "    0.17629331993972877]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 10.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.7189599999999999, 0.7308, 0.7315199999999998, 0.73424],\n",
       "   'test': [0.22792566549472631,\n",
       "    0.19005524861878456,\n",
       "    0.17780010045203418,\n",
       "    0.17408337518834757]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 10.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.7209599999999999, 0.73088, 0.7363999999999999, 0.73896],\n",
       "   'test': [0.22792566549472631,\n",
       "    0.18985434455047714,\n",
       "    0.1849321948769463,\n",
       "    0.17880462079357107]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 10.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.7155199999999999, 0.72664, 0.72712, 0.7288],\n",
       "   'test': [0.22792566549472631,\n",
       "    0.1942742340532396,\n",
       "    0.1822199899547966,\n",
       "    0.17709693621295833]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 10.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.7236799999999999, 0.72808, 0.7296799999999999, 0.7324],\n",
       "   'test': [0.2266197890507283,\n",
       "    0.19085886489201406,\n",
       "    0.18503264691109997,\n",
       "    0.17689603214465094]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 10.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.7165600000000001,\n",
       "    0.7220799999999999,\n",
       "    0.7261599999999999,\n",
       "    0.72992],\n",
       "   'test': [0.22641888498242094,\n",
       "    0.18844801607232547,\n",
       "    0.17699648417880462,\n",
       "    0.17388247112004018]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 10.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.7047999999999999, 0.7198399999999999, 0.72608, 0.73088],\n",
       "   'test': [0.22340532395781018,\n",
       "    0.1882471120040181,\n",
       "    0.17559015570065295,\n",
       "    0.1749874434957308]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 10.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.7109599999999999,\n",
       "    0.71872,\n",
       "    0.7232799999999999,\n",
       "    0.7256799999999999],\n",
       "   'test': [0.22340532395781015,\n",
       "    0.18975389251632346,\n",
       "    0.17960823706680062,\n",
       "    0.1750878955298845]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 10.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.6895199999999999, 0.7135199999999999, 0.71936, 0.72576],\n",
       "   'test': [0.22069311903566052,\n",
       "    0.18895027624309393,\n",
       "    0.17840281265695632,\n",
       "    0.17699648417880462]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 100.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.51008, 0.5328, 0.5463199999999999, 0.552],\n",
       "   'test': [0.18553490708186843,\n",
       "    0.19095931692616777,\n",
       "    0.19698643897538928,\n",
       "    0.19899547965846312]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 100.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.51168, 0.53408, 0.54976, 0.55304],\n",
       "   'test': [0.18653942742340535,\n",
       "    0.1929683576092416,\n",
       "    0.19849321948769463,\n",
       "    0.19748869914615774]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 100.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.5102399999999999,\n",
       "    0.5337599999999999,\n",
       "    0.5452800000000001,\n",
       "    0.5516],\n",
       "   'test': [0.18583626318432947,\n",
       "    0.1917629331993973,\n",
       "    0.1965846308387745,\n",
       "    0.1978905072827725]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 100.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.51088, 0.53152, 0.54424, 0.5516800000000001],\n",
       "   'test': [0.18663987945755903,\n",
       "    0.19095931692616777,\n",
       "    0.19638372677046712,\n",
       "    0.1977900552486188]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 100.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.5107200000000001, 0.53464, 0.54632, 0.5524],\n",
       "   'test': [0.18643897538925167,\n",
       "    0.1914615770969362,\n",
       "    0.19708689100954296,\n",
       "    0.1976896032144651]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 100.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.51064, 0.5342399999999999, 0.54864, 0.55232],\n",
       "   'test': [0.18643897538925167,\n",
       "    0.19236564540431944,\n",
       "    0.1976896032144651,\n",
       "    0.19758915118031142]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 100.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.51184, 0.53472, 0.54776, 0.55424],\n",
       "   'test': [0.18724259166248117,\n",
       "    0.19206428930185834,\n",
       "    0.1976896032144651,\n",
       "    0.197388247112004]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 100.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.5119199999999999, 0.53384, 0.5468, 0.55288],\n",
       "   'test': [0.18643897538925167,\n",
       "    0.19015570065293824,\n",
       "    0.19618282270215973,\n",
       "    0.197388247112004]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 100.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.51064, 0.53096, 0.54632, 0.55184],\n",
       "   'test': [0.1868407835258664,\n",
       "    0.19166248116524362,\n",
       "    0.19648417880462082,\n",
       "    0.19728779507785035]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.1, 'alpha': 500},\n",
       "  'patk': {'train': [0.67912, 0.69192, 0.7018399999999999, 0.7075199999999999],\n",
       "   'test': [0.19708689100954296,\n",
       "    0.16976393771973883,\n",
       "    0.1608237066800603,\n",
       "    0.1560020090406831]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 100.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.51032, 0.53208, 0.54912, 0.55224],\n",
       "   'test': [0.18583626318432947,\n",
       "    0.1919638372677047,\n",
       "    0.19698643897538926,\n",
       "    0.19688598694123555]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 100.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.5095200000000001, 0.52752, 0.5408, 0.5468],\n",
       "   'test': [0.1867403314917127,\n",
       "    0.19015570065293824,\n",
       "    0.19487694625816174,\n",
       "    0.19658463083877448]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 100.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.50968, 0.52928, 0.5428, 0.5493600000000001],\n",
       "   'test': [0.18553490708186843,\n",
       "    0.19116022099447516,\n",
       "    0.19527875439477646,\n",
       "    0.19648417880462082]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 100.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.51072, 0.52408, 0.53968, 0.54528],\n",
       "   'test': [0.18483174284279258,\n",
       "    0.1872425916624812,\n",
       "    0.1941737820190859,\n",
       "    0.19598191863385236]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 100.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.5115199999999999, 0.52376, 0.53872, 0.5437599999999999],\n",
       "   'test': [0.18553490708186843,\n",
       "    0.18583626318432947,\n",
       "    0.19236564540431944,\n",
       "    0.19578101456554492]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.1, 'alpha': 10},\n",
       "  'patk': {'train': [0.67856, 0.68504, 0.69416, 0.7044],\n",
       "   'test': [0.1950778503264691,\n",
       "    0.17338021094927175,\n",
       "    0.1594173782019086,\n",
       "    0.15519839276745354]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.1, 'alpha': 1},\n",
       "  'patk': {'train': [0.67568, 0.6871200000000001, 0.69896, 0.70552],\n",
       "   'test': [0.19487694625816177,\n",
       "    0.17046710195881468,\n",
       "    0.1626318432948267,\n",
       "    0.15700652938222]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 100.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.51184, 0.52304, 0.53752, 0.54376],\n",
       "   'test': [0.18553490708186843,\n",
       "    0.18583626318432947,\n",
       "    0.191863385233551,\n",
       "    0.19487694625816174]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.1, 'alpha': 10},\n",
       "  'patk': {'train': [0.674,\n",
       "    0.6907200000000001,\n",
       "    0.7003199999999999,\n",
       "    0.7051999999999999],\n",
       "   'test': [0.19457559015570067,\n",
       "    0.17277749874434958,\n",
       "    0.1604218985434455,\n",
       "    0.1589151180311401]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 100.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.5104,\n",
       "    0.5262399999999999,\n",
       "    0.5404800000000001,\n",
       "    0.5467200000000001],\n",
       "   'test': [0.18643897538925167,\n",
       "    0.1883475640381718,\n",
       "    0.1943746860873933,\n",
       "    0.19457559015570064]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.1, 'alpha': 50},\n",
       "  'patk': {'train': [0.67728, 0.69176, 0.70192, 0.70896],\n",
       "   'test': [0.1941737820190859,\n",
       "    0.1735811150175791,\n",
       "    0.16614766449020593,\n",
       "    0.16022099447513813]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 100.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.5115200000000001,\n",
       "    0.5227200000000001,\n",
       "    0.5374399999999999,\n",
       "    0.54256],\n",
       "   'test': [0.1856353591160221,\n",
       "    0.18663987945755903,\n",
       "    0.19206428930185837,\n",
       "    0.1940733299849322]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 100.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.5124799999999999, 0.52272, 0.53472, 0.5420799999999999],\n",
       "   'test': [0.18623807132094428,\n",
       "    0.18613761928679057,\n",
       "    0.19025615268709192,\n",
       "    0.1940733299849322]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 100.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.5107200000000001, 0.52064, 0.5352, 0.5422399999999999],\n",
       "   'test': [0.18533400301356104,\n",
       "    0.1856353591160221,\n",
       "    0.18965344048216978,\n",
       "    0.19387242591662485]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 100.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.5110399999999999, 0.52232, 0.53632, 0.54168],\n",
       "   'test': [0.1859367152184832,\n",
       "    0.18583626318432947,\n",
       "    0.19156202913108988,\n",
       "    0.19377197388247114]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 100.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.51176, 0.52312, 0.53448, 0.53984],\n",
       "   'test': [0.18412857860371673,\n",
       "    0.1868407835258664,\n",
       "    0.1903566047212456,\n",
       "    0.19357106981416375]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 100.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.51304, 0.522, 0.5344, 0.5408],\n",
       "   'test': [0.1868407835258664,\n",
       "    0.18643897538925167,\n",
       "    0.19085886489201406,\n",
       "    0.19357106981416372]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 100.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.5112, 0.5208, 0.5343200000000001, 0.5415200000000001],\n",
       "   'test': [0.18503264691109997,\n",
       "    0.18412857860371673,\n",
       "    0.18895027624309393,\n",
       "    0.19347061778001007]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.1, 'alpha': 1},\n",
       "  'patk': {'train': [0.67504, 0.68712, 0.69792, 0.70352],\n",
       "   'test': [0.1932697137117027,\n",
       "    0.1684580612757408,\n",
       "    0.1604218985434455,\n",
       "    0.15389251632345555]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.1, 'alpha': 1000},\n",
       "  'patk': {'train': [0.6707200000000001, 0.68616, 0.69704, 0.70656],\n",
       "   'test': [0.1930688096433953,\n",
       "    0.16695128076343546,\n",
       "    0.15560020090406834,\n",
       "    0.1512807634354596]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.1, 'alpha': 100},\n",
       "  'patk': {'train': [0.6768799999999999, 0.6919199999999999, 0.6976, 0.70224],\n",
       "   'test': [0.19306880964339526,\n",
       "    0.16956303365143147,\n",
       "    0.15891511803114014,\n",
       "    0.1582119537920643]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.1, 'alpha': 1},\n",
       "  'patk': {'train': [0.6748, 0.6928, 0.70304, 0.7088],\n",
       "   'test': [0.1929683576092416,\n",
       "    0.17066800602712207,\n",
       "    0.16313410346559515,\n",
       "    0.15811150175791058]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 100.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.5129600000000001,\n",
       "    0.52184,\n",
       "    0.5333600000000001,\n",
       "    0.5393600000000001],\n",
       "   'test': [0.18453038674033154,\n",
       "    0.18412857860371673,\n",
       "    0.18915118031140132,\n",
       "    0.1928679055750879]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.1, 'alpha': 100},\n",
       "  'patk': {'train': [0.6704, 0.68576, 0.69824, 0.704],\n",
       "   'test': [0.1926670015067805,\n",
       "    0.17297840281265695,\n",
       "    0.166449020592667,\n",
       "    0.1592164741336012]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.1, 'alpha': 1},\n",
       "  'patk': {'train': [0.6813600000000001, 0.69352, 0.69936, 0.70344],\n",
       "   'test': [0.19256654947262683,\n",
       "    0.17096936212958316,\n",
       "    0.15981918633852335,\n",
       "    0.1556002009040683]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.1, 'alpha': 10},\n",
       "  'patk': {'train': [0.6697599999999999,\n",
       "    0.6824,\n",
       "    0.6926399999999999,\n",
       "    0.7021599999999999],\n",
       "   'test': [0.19246609743847315,\n",
       "    0.1712707182320442,\n",
       "    0.15931692616775492,\n",
       "    0.15650426921145155]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.1, 'alpha': 100},\n",
       "  'patk': {'train': [0.6788, 0.68992, 0.6980799999999999, 0.7075199999999999],\n",
       "   'test': [0.19246609743847312,\n",
       "    0.16946258161727773,\n",
       "    0.16484178804620797,\n",
       "    0.16233048719236567]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 100.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.51144, 0.52152, 0.53424, 0.53984],\n",
       "   'test': [0.1849321948769463,\n",
       "    0.1846308387744852,\n",
       "    0.18965344048216978,\n",
       "    0.19216474133601208]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 100.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.51192, 0.52056, 0.5335200000000001, 0.53992],\n",
       "   'test': [0.1856353591160221,\n",
       "    0.18402812656956302,\n",
       "    0.18995479658463083,\n",
       "    0.19216474133601208]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.1, 'alpha': 10},\n",
       "  'patk': {'train': [0.6785599999999999,\n",
       "    0.6843199999999999,\n",
       "    0.69472,\n",
       "    0.7044799999999999],\n",
       "   'test': [0.19166248116524362,\n",
       "    0.16614766449020593,\n",
       "    0.1558011049723757,\n",
       "    0.15308890005022602]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.1, 'alpha': 500},\n",
       "  'patk': {'train': [0.67224, 0.68704, 0.6946399999999999, 0.7051200000000001],\n",
       "   'test': [0.19166248116524362,\n",
       "    0.16976393771973883,\n",
       "    0.16192867905575087,\n",
       "    0.1605223505775992]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 100.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.51208, 0.51984, 0.5314399999999999, 0.5382399999999999],\n",
       "   'test': [0.1846308387744852,\n",
       "    0.18442993470617783,\n",
       "    0.18854846810647918,\n",
       "    0.19105976896032142]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.1, 'alpha': 500},\n",
       "  'patk': {'train': [0.6732, 0.69024, 0.6988, 0.70712],\n",
       "   'test': [0.19085886489201406,\n",
       "    0.17046710195881465,\n",
       "    0.1617277749874435,\n",
       "    0.1581115017579106]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.1, 'alpha': 1000},\n",
       "  'patk': {'train': [0.67088, 0.6906399999999999, 0.6984799999999999, 0.70056],\n",
       "   'test': [0.1906579608237067,\n",
       "    0.1655449522852838,\n",
       "    0.1537920642893019,\n",
       "    0.15178302360622806]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 100.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.5156799999999999, 0.52056, 0.53056, 0.5367200000000001],\n",
       "   'test': [0.1845303867403315,\n",
       "    0.18442993470617783,\n",
       "    0.18854846810647916,\n",
       "    0.1904570567553993]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.1, 'alpha': 100},\n",
       "  'patk': {'train': [0.67016, 0.68616, 0.69624, 0.70416],\n",
       "   'test': [0.1903566047212456,\n",
       "    0.16896032144650927,\n",
       "    0.1629331993972878,\n",
       "    0.158312405826218]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 100.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.51344, 0.5208, 0.53208, 0.5381600000000001],\n",
       "   'test': [0.1847312908086389,\n",
       "    0.1845303867403315,\n",
       "    0.18895027624309393,\n",
       "    0.19015570065293824]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 100.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.5160799999999999, 0.52112, 0.5308799999999999, 0.53584],\n",
       "   'test': [0.18453038674033148,\n",
       "    0.1867403314917127,\n",
       "    0.18824711200401809,\n",
       "    0.18995479658463085]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.1, 'alpha': 50},\n",
       "  'patk': {'train': [0.68032, 0.69432, 0.7004, 0.70592],\n",
       "   'test': [0.18985434455047717,\n",
       "    0.1698643897538925,\n",
       "    0.16082370668006027,\n",
       "    0.15770969362129583]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.1, 'alpha': 500},\n",
       "  'patk': {'train': [0.6854399999999999,\n",
       "    0.7000799999999999,\n",
       "    0.7063200000000001,\n",
       "    0.71336],\n",
       "   'test': [0.1895529884480161,\n",
       "    0.16695128076343546,\n",
       "    0.15991963837267706,\n",
       "    0.15549974886991463]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.1, 'alpha': 1000},\n",
       "  'patk': {'train': [0.67384, 0.6916, 0.70424, 0.7116],\n",
       "   'test': [0.189251632345555,\n",
       "    0.16574585635359115,\n",
       "    0.15881466599698646,\n",
       "    0.15509794073329985]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.1, 'alpha': 50},\n",
       "  'patk': {'train': [0.67704, 0.6889599999999999, 0.70168, 0.7098399999999999],\n",
       "   'test': [0.1889502762430939,\n",
       "    0.1661476644902059,\n",
       "    0.16122551481667505,\n",
       "    0.1571069814163737]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.1, 'alpha': 50},\n",
       "  'patk': {'train': [0.6648, 0.68624, 0.69544, 0.7048],\n",
       "   'test': [0.18834756403817177,\n",
       "    0.16604721245605225,\n",
       "    0.1585133098945254,\n",
       "    0.1548970366649925]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.1, 'alpha': 500},\n",
       "  'patk': {'train': [0.676, 0.6913600000000001, 0.69944, 0.70472],\n",
       "   'test': [0.1878453038674033,\n",
       "    0.16604721245605225,\n",
       "    0.15891511803114014,\n",
       "    0.15529884480160724]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.1, 'alpha': 100},\n",
       "  'patk': {'train': [0.67032, 0.6856, 0.6979199999999999, 0.70288],\n",
       "   'test': [0.18764439979909595,\n",
       "    0.17036664992466097,\n",
       "    0.1616273229532898,\n",
       "    0.15841285786037168]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.1, 'alpha': 1000},\n",
       "  'patk': {'train': [0.66752, 0.6892, 0.6968799999999999, 0.7052],\n",
       "   'test': [0.18734304369663485,\n",
       "    0.1749874434957308,\n",
       "    0.16614766449020593,\n",
       "    0.15881466599698646]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.1, 'alpha': 1000},\n",
       "  'patk': {'train': [0.67304, 0.6839999999999999, 0.6960799999999999, 0.70512],\n",
       "   'test': [0.1871421396283275,\n",
       "    0.17086891009542945,\n",
       "    0.159618282270216,\n",
       "    0.1581115017579106]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.1, 'alpha': 1},\n",
       "  'patk': {'train': [0.668, 0.6854399999999999, 0.69912, 0.7060799999999999],\n",
       "   'test': [0.1871421396283275,\n",
       "    0.16825715720743345,\n",
       "    0.15760924158714215,\n",
       "    0.1533902561526871]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.1, 'alpha': 10},\n",
       "  'patk': {'train': [0.67096, 0.6839200000000001, 0.6963199999999999, 0.70568],\n",
       "   'test': [0.18533400301356104,\n",
       "    0.16715218483174288,\n",
       "    0.1592164741336012,\n",
       "    0.154394776494224]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.1, 'alpha': 50},\n",
       "  'patk': {'train': [0.6799199999999999, 0.68928, 0.70288, 0.70968],\n",
       "   'test': [0.18402812656956305,\n",
       "    0.16725263686589653,\n",
       "    0.16182822702159722,\n",
       "    0.15640381717729782]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.66216, 0.6928799999999999, 0.7124, 0.72504],\n",
       "   'test': [0.18292315419387242,\n",
       "    0.15730788548468103,\n",
       "    0.1529884480160723,\n",
       "    0.15208437970868907]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.66752, 0.69816, 0.7126399999999999, 0.7248],\n",
       "   'test': [0.18292315419387242,\n",
       "    0.16323455549974886,\n",
       "    0.15288799598191863,\n",
       "    0.1511803114013059]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.001, 'alpha': 10},\n",
       "  'patk': {'train': [0.6668, 0.6971999999999999, 0.70968, 0.7184799999999999],\n",
       "   'test': [0.18242089402310396,\n",
       "    0.1639377197388247,\n",
       "    0.15610246107483677,\n",
       "    0.15228528377699652]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 1e-05, 'alpha': 100},\n",
       "  'patk': {'train': [0.6664800000000001,\n",
       "    0.69224,\n",
       "    0.7045600000000001,\n",
       "    0.7139999999999999],\n",
       "   'test': [0.1820190858864892,\n",
       "    0.15901557006529382,\n",
       "    0.15328980411853343,\n",
       "    0.14595680562531393]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.001, 'alpha': 500},\n",
       "  'patk': {'train': [0.6661600000000001,\n",
       "    0.6957599999999999,\n",
       "    0.7127199999999999,\n",
       "    0.7241599999999999],\n",
       "   'test': [0.18181818181818182,\n",
       "    0.15991963837267703,\n",
       "    0.1507785032646911,\n",
       "    0.1511803114013059]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.001, 'alpha': 1000},\n",
       "  'patk': {'train': [0.6642399999999999,\n",
       "    0.69616,\n",
       "    0.7124799999999999,\n",
       "    0.7224799999999999],\n",
       "   'test': [0.18181818181818182,\n",
       "    0.1607232546459066,\n",
       "    0.1571069814163737,\n",
       "    0.15308890005022602]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 1e-05, 'alpha': 1000},\n",
       "  'patk': {'train': [0.6647200000000001,\n",
       "    0.6932799999999999,\n",
       "    0.70672,\n",
       "    0.7156799999999999],\n",
       "   'test': [0.18171772978402814,\n",
       "    0.15740833751883476,\n",
       "    0.15419387242591662,\n",
       "    0.15087895529884482]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 1e-05, 'alpha': 1},\n",
       "  'patk': {'train': [0.66384, 0.69504, 0.71144, 0.71936],\n",
       "   'test': [0.18151682571572075,\n",
       "    0.15951783023606228,\n",
       "    0.1522852837769965,\n",
       "    0.14957307885484686]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.001, 'alpha': 50},\n",
       "  'patk': {'train': [0.66272, 0.69624, 0.71472, 0.72624],\n",
       "   'test': [0.1812154696132597,\n",
       "    0.16122551481667505,\n",
       "    0.15750878955298847,\n",
       "    0.15519839276745354]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.6624, 0.692, 0.70888, 0.7187199999999999],\n",
       "   'test': [0.18051230537418383,\n",
       "    0.1619286790557509,\n",
       "    0.15459568056253142,\n",
       "    0.14876946258161727]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.001, 'alpha': 1000},\n",
       "  'patk': {'train': [0.6636799999999999, 0.6952799999999999, 0.7088, 0.71544],\n",
       "   'test': [0.1805123053741838,\n",
       "    0.15991963837267703,\n",
       "    0.15369161225514816,\n",
       "    0.1487694625816173]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.001, 'alpha': 100},\n",
       "  'patk': {'train': [0.66696, 0.6984799999999999, 0.71288, 0.72088],\n",
       "   'test': [0.18031140130587645,\n",
       "    0.16494224008036162,\n",
       "    0.15590155700652938,\n",
       "    0.1511803114013059]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.6674399999999999,\n",
       "    0.6911199999999998,\n",
       "    0.7079199999999999,\n",
       "    0.72144],\n",
       "   'test': [0.18011049723756908,\n",
       "    0.1592164741336012,\n",
       "    0.1518834756403817,\n",
       "    0.15047714716223004]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 1e-05, 'alpha': 100},\n",
       "  'patk': {'train': [0.6656799999999999, 0.6984, 0.71336, 0.72304],\n",
       "   'test': [0.1800100452034154,\n",
       "    0.1606228026117529,\n",
       "    0.1518834756403817,\n",
       "    0.14816675037669513]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.001, 'alpha': 1},\n",
       "  'patk': {'train': [0.66592, 0.69584, 0.71232, 0.7238399999999999],\n",
       "   'test': [0.18001004520341538,\n",
       "    0.16102461074836766,\n",
       "    0.15439477649422406,\n",
       "    0.15399296835760926]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.6608799999999999, 0.69192, 0.71024, 0.71992],\n",
       "   'test': [0.1799095931692617,\n",
       "    0.16223003515821197,\n",
       "    0.1545956805625314,\n",
       "    0.1508789552988448]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.66584, 0.69792, 0.71256, 0.72136],\n",
       "   'test': [0.1799095931692617,\n",
       "    0.1604218985434455,\n",
       "    0.1531893520843797,\n",
       "    0.15298844801607234]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.001, 'alpha': 500},\n",
       "  'patk': {'train': [0.66392, 0.69568, 0.71424, 0.7244799999999999],\n",
       "   'test': [0.1799095931692617,\n",
       "    0.1607232546459066,\n",
       "    0.1534907081868408,\n",
       "    0.15288799598191866]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.66496, 0.6979199999999999, 0.7124799999999999, 0.72136],\n",
       "   'test': [0.17980914113510799,\n",
       "    0.16102461074836766,\n",
       "    0.15248618784530388,\n",
       "    0.1512807634354596]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 1e-05, 'alpha': 1},\n",
       "  'patk': {'train': [0.6656, 0.69968, 0.7156799999999999, 0.72488],\n",
       "   'test': [0.17980914113510799,\n",
       "    0.1578101456554495,\n",
       "    0.1531893520843797,\n",
       "    0.1495730788548468]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 1e-05, 'alpha': 10},\n",
       "  'patk': {'train': [0.6647199999999999, 0.69592, 0.7135999999999999, 0.72216],\n",
       "   'test': [0.1795077850326469,\n",
       "    0.16102461074836766,\n",
       "    0.15449522852837771,\n",
       "    0.15359116022099448]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.6651199999999999, 0.6964, 0.71504, 0.72408],\n",
       "   'test': [0.17940733299849324,\n",
       "    0.16092415871421398,\n",
       "    0.1582119537920643,\n",
       "    0.1534907081868408]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.001, 'alpha': 500},\n",
       "  'patk': {'train': [0.6644800000000001, 0.69368, 0.7095199999999999, 0.71864],\n",
       "   'test': [0.17940733299849324,\n",
       "    0.15901557006529382,\n",
       "    0.15148166750376696,\n",
       "    0.1486690105474636]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 1e-05, 'alpha': 1},\n",
       "  'patk': {'train': [0.66616, 0.6949599999999998, 0.7111199999999999, 0.72224],\n",
       "   'test': [0.17940733299849324,\n",
       "    0.16243093922651936,\n",
       "    0.15459568056253142,\n",
       "    0.15308890005022602]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.001, 'alpha': 1000},\n",
       "  'patk': {'train': [0.66496, 0.69512, 0.71152, 0.72192],\n",
       "   'test': [0.17930688096433953,\n",
       "    0.16233048719236567,\n",
       "    0.15680562531391262,\n",
       "    0.1532898041185334]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 1e-05, 'alpha': 500},\n",
       "  'patk': {'train': [0.66616,\n",
       "    0.6989599999999999,\n",
       "    0.7147199999999999,\n",
       "    0.7253599999999999],\n",
       "   'test': [0.17920642893018585,\n",
       "    0.15901557006529382,\n",
       "    0.1537920642893019,\n",
       "    0.15218483174284278]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.6624, 0.69072, 0.7096, 0.7215199999999999],\n",
       "   'test': [0.17920642893018585,\n",
       "    0.16383726770467105,\n",
       "    0.15720743345052737,\n",
       "    0.15339025615268712]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 1e-05, 'alpha': 100},\n",
       "  'patk': {'train': [0.66176, 0.69224, 0.7076, 0.7184],\n",
       "   'test': [0.17900552486187843,\n",
       "    0.16152687091913612,\n",
       "    0.15740833751883473,\n",
       "    0.15268709191361124]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 1e-05, 'alpha': 500},\n",
       "  'patk': {'train': [0.6660799999999999,\n",
       "    0.6963199999999999,\n",
       "    0.7138399999999999,\n",
       "    0.72544],\n",
       "   'test': [0.17890507282772475,\n",
       "    0.15861376192867904,\n",
       "    0.15610246107483675,\n",
       "    0.15409342039176296]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 1e-05, 'alpha': 1000},\n",
       "  'patk': {'train': [0.66872, 0.69768, 0.71296, 0.7212799999999999],\n",
       "   'test': [0.17880462079357107,\n",
       "    0.16102461074836766,\n",
       "    0.15268709191361124,\n",
       "    0.15138121546961328]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.6640799999999999,\n",
       "    0.69792,\n",
       "    0.7147999999999999,\n",
       "    0.7236799999999999],\n",
       "   'test': [0.17880462079357107,\n",
       "    0.16373681567051732,\n",
       "    0.15650426921145155,\n",
       "    0.1523857358111502]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.66464, 0.6964, 0.7117600000000001, 0.7215199999999999],\n",
       "   'test': [0.1787041687594174,\n",
       "    0.1600200904068307,\n",
       "    0.1534907081868408,\n",
       "    0.1511803114013059]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.6656, 0.6944799999999999, 0.7107199999999999, 0.72016],\n",
       "   'test': [0.17850326469111003,\n",
       "    0.16313410346559518,\n",
       "    0.15720743345052737,\n",
       "    0.1531893520843797]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 1e-05, 'alpha': 500},\n",
       "  'patk': {'train': [0.6648799999999999,\n",
       "    0.6932799999999999,\n",
       "    0.7127199999999999,\n",
       "    0.72264],\n",
       "   'test': [0.17840281265695632,\n",
       "    0.16122551481667505,\n",
       "    0.15499748869914617,\n",
       "    0.15359116022099448]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.001, 'alpha': 1000},\n",
       "  'patk': {'train': [0.66128, 0.6945600000000001, 0.71224, 0.71904],\n",
       "   'test': [0.17840281265695632,\n",
       "    0.16403817177297841,\n",
       "    0.1581115017579106,\n",
       "    0.15499748869914617]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 1e-05, 'alpha': 1000},\n",
       "  'patk': {'train': [0.66504, 0.69736, 0.71176, 0.72304],\n",
       "   'test': [0.17840281265695632,\n",
       "    0.1585133098945254,\n",
       "    0.1537920642893019,\n",
       "    0.15439477649422403]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.001, 'alpha': 1},\n",
       "  'patk': {'train': [0.6596, 0.6912, 0.70816, 0.72192],\n",
       "   'test': [0.1784028126569563,\n",
       "    0.16283274736313413,\n",
       "    0.15499748869914617,\n",
       "    0.15479658463083878]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.001, 'alpha': 50},\n",
       "  'patk': {'train': [0.6588799999999999, 0.68616, 0.70136, 0.71136],\n",
       "   'test': [0.17820190858864896,\n",
       "    0.15781014565544954,\n",
       "    0.15258663987945756,\n",
       "    0.14907081868407837]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.001, 'alpha': 10},\n",
       "  'patk': {'train': [0.66384, 0.6944, 0.7115199999999999, 0.72],\n",
       "   'test': [0.17820190858864893,\n",
       "    0.15760924158714215,\n",
       "    0.14886991461577098,\n",
       "    0.14535409342039174]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.65608, 0.6894399999999999, 0.70872, 0.7212799999999999],\n",
       "   'test': [0.17810145655449525,\n",
       "    0.15991963837267706,\n",
       "    0.15469613259668508,\n",
       "    0.14917127071823205]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 1e-05, 'alpha': 10},\n",
       "  'patk': {'train': [0.66648, 0.69472, 0.7131199999999999, 0.72312],\n",
       "   'test': [0.17810145655449525,\n",
       "    0.15991963837267703,\n",
       "    0.15107985936715218,\n",
       "    0.15268709191361124]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.6631199999999999, 0.6956, 0.71216, 0.72168],\n",
       "   'test': [0.17800100452034154,\n",
       "    0.16343545956805627,\n",
       "    0.15389251632345555,\n",
       "    0.1509794073329985]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 1e-05, 'alpha': 10},\n",
       "  'patk': {'train': [0.6633600000000001,\n",
       "    0.6924799999999999,\n",
       "    0.70496,\n",
       "    0.7147199999999999],\n",
       "   'test': [0.17800100452034154,\n",
       "    0.16313410346559518,\n",
       "    0.15258663987945756,\n",
       "    0.14786539427423404]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 1e-05, 'alpha': 50},\n",
       "  'patk': {'train': [0.6631999999999999, 0.694, 0.70544, 0.7144799999999999],\n",
       "   'test': [0.17780010045203415,\n",
       "    0.1611250627825213,\n",
       "    0.1537920642893019,\n",
       "    0.1511803114013059]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.001, 'alpha': 1},\n",
       "  'patk': {'train': [0.66752, 0.6923199999999999, 0.7068, 0.7164],\n",
       "   'test': [0.17780010045203415,\n",
       "    0.15941737820190857,\n",
       "    0.15409342039176294,\n",
       "    0.15218483174284278]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.001, 'alpha': 1},\n",
       "  'patk': {'train': [0.6631199999999999,\n",
       "    0.6924799999999999,\n",
       "    0.7059199999999999,\n",
       "    0.7144],\n",
       "   'test': [0.1776996484178805,\n",
       "    0.165143144148669,\n",
       "    0.1579105976896032,\n",
       "    0.15389251632345555]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.66096, 0.68784, 0.70824, 0.7176799999999999],\n",
       "   'test': [0.17769964841788047,\n",
       "    0.16042189854344552,\n",
       "    0.15057759919638375,\n",
       "    0.15148166750376696]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.6605599999999999,\n",
       "    0.68816,\n",
       "    0.7078399999999999,\n",
       "    0.7184799999999999],\n",
       "   'test': [0.17769964841788047,\n",
       "    0.15931692616775492,\n",
       "    0.15218483174284278,\n",
       "    0.15077850326469114]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.001, 'alpha': 50},\n",
       "  'patk': {'train': [0.6662399999999999, 0.69584, 0.7124, 0.7212799999999999],\n",
       "   'test': [0.17769964841788047,\n",
       "    0.16202913108990458,\n",
       "    0.15519839276745354,\n",
       "    0.15459568056253137]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.66392, 0.69176, 0.7084, 0.7152],\n",
       "   'test': [0.1775991963837268,\n",
       "    0.16122551481667505,\n",
       "    0.15499748869914617,\n",
       "    0.15037669512807633]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 1e-05, 'alpha': 1000},\n",
       "  'patk': {'train': [0.66864, 0.6949599999999999, 0.71016, 0.71904],\n",
       "   'test': [0.1775991963837268,\n",
       "    0.16062280261175285,\n",
       "    0.15228528377699652,\n",
       "    0.1487694625816173]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 1e-05, 'alpha': 1},\n",
       "  'patk': {'train': [0.66864, 0.69576, 0.7107199999999999, 0.72064],\n",
       "   'test': [0.1774987443495731,\n",
       "    0.16243093922651936,\n",
       "    0.1560020090406831,\n",
       "    0.15238573581115017]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.66184, 0.6906399999999999, 0.70536, 0.71496],\n",
       "   'test': [0.1774987443495731,\n",
       "    0.16383726770467102,\n",
       "    0.15509794073329985,\n",
       "    0.15158211953792067]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.66336, 0.69016, 0.70624, 0.71944],\n",
       "   'test': [0.177197388247112,\n",
       "    0.16092415871421395,\n",
       "    0.15248618784530388,\n",
       "    0.14977398292315422]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 1e-05, 'alpha': 50},\n",
       "  'patk': {'train': [0.6591199999999999, 0.69224, 0.70992, 0.72016],\n",
       "   'test': [0.177197388247112,\n",
       "    0.16403817177297841,\n",
       "    0.15499748869914615,\n",
       "    0.1495730788548468]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.66272, 0.6924, 0.7091200000000001, 0.72024],\n",
       "   'test': [0.177197388247112,\n",
       "    0.15991963837267706,\n",
       "    0.15288799598191866,\n",
       "    0.14987443495730787]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.6634399999999999,\n",
       "    0.6964799999999999,\n",
       "    0.7115199999999999,\n",
       "    0.71824],\n",
       "   'test': [0.17719738824711198,\n",
       "    0.16122551481667505,\n",
       "    0.15308890005022602,\n",
       "    0.15168257157207435]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 1e-05, 'alpha': 10},\n",
       "  'patk': {'train': [0.6636799999999999, 0.69072, 0.7058399999999999, 0.71632],\n",
       "   'test': [0.17709693621295833,\n",
       "    0.15931692616775492,\n",
       "    0.1558011049723757,\n",
       "    0.15369161225514816]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.66432, 0.6948799999999999, 0.71216, 0.7212799999999999],\n",
       "   'test': [0.17699648417880465,\n",
       "    0.15680562531391262,\n",
       "    0.15369161225514816,\n",
       "    0.1509794073329985]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.001, 'alpha': 10},\n",
       "  'patk': {'train': [0.6676, 0.6954400000000001, 0.7124799999999999, 0.72112],\n",
       "   'test': [0.17679558011049723,\n",
       "    0.15841285786037168,\n",
       "    0.15871421396283275,\n",
       "    0.15469613259668508]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.0, 'alpha': 500},\n",
       "  'patk': {'train': [0.66376,\n",
       "    0.6944799999999999,\n",
       "    0.7129599999999999,\n",
       "    0.7264799999999999],\n",
       "   'test': [0.17669512807634358,\n",
       "    0.16273229532898043,\n",
       "    0.15529884480160724,\n",
       "    0.1534907081868408]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 1e-05, 'alpha': 50},\n",
       "  'patk': {'train': [0.6656, 0.69768, 0.71392, 0.72192],\n",
       "   'test': [0.1764942240080362,\n",
       "    0.1608237066800603,\n",
       "    0.15620291310899045,\n",
       "    0.15308890005022605]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.66184, 0.6944, 0.70824, 0.7175199999999999],\n",
       "   'test': [0.17649422400803616,\n",
       "    0.16122551481667505,\n",
       "    0.15529884480160724,\n",
       "    0.15077850326469114]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.001, 'alpha': 1},\n",
       "  'patk': {'train': [0.66432, 0.6952, 0.7117599999999998, 0.71776],\n",
       "   'test': [0.1762933199397288,\n",
       "    0.15951783023606228,\n",
       "    0.15690607734806628,\n",
       "    0.15248618784530388]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.001, 'alpha': 500},\n",
       "  'patk': {'train': [0.66224, 0.6916, 0.7060799999999999, 0.7196],\n",
       "   'test': [0.17629331993972877,\n",
       "    0.15750878955298847,\n",
       "    0.15580110497237568,\n",
       "    0.1511803114013059]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 1e-05, 'alpha': 10},\n",
       "  'patk': {'train': [0.6652, 0.69368, 0.7125600000000001, 0.7224799999999999],\n",
       "   'test': [0.17629331993972877,\n",
       "    0.158312405826218,\n",
       "    0.153088900050226,\n",
       "    0.14897036664992466]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.001, 'alpha': 50},\n",
       "  'patk': {'train': [0.66704, 0.6929599999999999, 0.71056, 0.72224],\n",
       "   'test': [0.17619286790557512,\n",
       "    0.1604218985434455,\n",
       "    0.15620291310899045,\n",
       "    0.15238573581115017]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.0, 'alpha': 100},\n",
       "  'patk': {'train': [0.66616, 0.6946399999999999, 0.70936, 0.7215199999999999],\n",
       "   'test': [0.1757910597689603,\n",
       "    0.16202913108990458,\n",
       "    0.1534907081868408,\n",
       "    0.15047714716223004]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.0, 'alpha': 50},\n",
       "  'patk': {'train': [0.664, 0.69392, 0.70968, 0.7166399999999999],\n",
       "   'test': [0.17569060773480666,\n",
       "    0.1571069814163737,\n",
       "    0.15027624309392265,\n",
       "    0.144952285283777]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.001, 'alpha': 100},\n",
       "  'patk': {'train': [0.66632, 0.69376, 0.7092799999999999, 0.71872],\n",
       "   'test': [0.17569060773480663,\n",
       "    0.16122551481667505,\n",
       "    0.15560020090406831,\n",
       "    0.15419387242591662]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 1e-05, 'alpha': 1},\n",
       "  'patk': {'train': [0.6641600000000001, 0.69736, 0.71424, 0.7252],\n",
       "   'test': [0.17548970366649927,\n",
       "    0.1604218985434455,\n",
       "    0.1540934203917629,\n",
       "    0.15148166750376696]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.001, 'alpha': 100},\n",
       "  'patk': {'train': [0.66296, 0.69376, 0.7084799999999999, 0.7171199999999999],\n",
       "   'test': [0.17548970366649927,\n",
       "    0.15791059768960322,\n",
       "    0.15298844801607234,\n",
       "    0.14917127071823205]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 1e-05, 'alpha': 500},\n",
       "  'patk': {'train': [0.6671199999999999, 0.6992799999999999, 0.71408, 0.72456],\n",
       "   'test': [0.1753892516323456,\n",
       "    0.16022099447513813,\n",
       "    0.15519839276745354,\n",
       "    0.1520843797086891]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.001, 'alpha': 50},\n",
       "  'patk': {'train': [0.65712, 0.6904, 0.70968, 0.7196],\n",
       "   'test': [0.1753892516323456,\n",
       "    0.15871421396283275,\n",
       "    0.15218483174284278,\n",
       "    0.15198392767453542]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 1e-05, 'alpha': 500},\n",
       "  'patk': {'train': [0.6668, 0.6976, 0.7132799999999999, 0.7212799999999999],\n",
       "   'test': [0.17528879959819185,\n",
       "    0.159618282270216,\n",
       "    0.15419387242591662,\n",
       "    0.15168257157207435]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.001, 'alpha': 500},\n",
       "  'patk': {'train': [0.66272, 0.69016, 0.71024, 0.72032],\n",
       "   'test': [0.17518834756403817,\n",
       "    0.15871421396283275,\n",
       "    0.15560020090406831,\n",
       "    0.1512807634354596]}},\n",
       " {'params': {'num_factors': 80, 'regularization': 0.001, 'alpha': 10},\n",
       "  'patk': {'train': [0.66448, 0.6937599999999999, 0.71168, 0.72064],\n",
       "   'test': [0.17478653942742342,\n",
       "    0.15961828227021596,\n",
       "    0.15067805123053743,\n",
       "    0.14897036664992466]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 1e-05, 'alpha': 50},\n",
       "  'patk': {'train': [0.6642399999999999, 0.69824, 0.714, 0.72176],\n",
       "   'test': [0.1746860873932697,\n",
       "    0.1652435961828227,\n",
       "    0.15620291310899045,\n",
       "    0.1545956805625314]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.001, 'alpha': 1000},\n",
       "  'patk': {'train': [0.66688,\n",
       "    0.6997599999999999,\n",
       "    0.7169599999999999,\n",
       "    0.7236799999999999],\n",
       "   'test': [0.1746860873932697,\n",
       "    0.15891511803114014,\n",
       "    0.1558011049723757,\n",
       "    0.15328980411853343]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.001, 'alpha': 10},\n",
       "  'patk': {'train': [0.66304, 0.6964799999999999, 0.7120799999999999, 0.72144],\n",
       "   'test': [0.1746860873932697,\n",
       "    0.1618282270215972,\n",
       "    0.15429432446007033,\n",
       "    0.15148166750376696]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 1e-05, 'alpha': 1000},\n",
       "  'patk': {'train': [0.6593600000000001, 0.69152, 0.7072, 0.71544],\n",
       "   'test': [0.17458563535911603,\n",
       "    0.16403817177297841,\n",
       "    0.15509794073329983,\n",
       "    0.1511803114013059]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.0, 'alpha': 10},\n",
       "  'patk': {'train': [0.6632, 0.69504, 0.70904, 0.7185600000000001],\n",
       "   'test': [0.17458563535911603,\n",
       "    0.16132596685082876,\n",
       "    0.15630336514314414,\n",
       "    0.15067805123053743]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.66472, 0.6953600000000001, 0.7108, 0.7189599999999999],\n",
       "   'test': [0.17438473129080867,\n",
       "    0.1585133098945254,\n",
       "    0.15389251632345557,\n",
       "    0.14746358613761928]}},\n",
       " {'params': {'num_factors': 20, 'regularization': 0.0, 'alpha': 1},\n",
       "  'patk': {'train': [0.66104, 0.6967199999999999, 0.71296, 0.72312],\n",
       "   'test': [0.17428427925665493,\n",
       "    0.1592164741336012,\n",
       "    0.15107985936715218,\n",
       "    0.15077850326469114]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 0.0, 'alpha': 1000},\n",
       "  'patk': {'train': [0.6643199999999999, 0.6916, 0.7075199999999999, 0.71792],\n",
       "   'test': [0.17428427925665493,\n",
       "    0.15449522852837771,\n",
       "    0.15128076343545957,\n",
       "    0.14776494224008035]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 0.001, 'alpha': 100},\n",
       "  'patk': {'train': [0.6574399999999999,\n",
       "    0.6886399999999999,\n",
       "    0.7044,\n",
       "    0.7118399999999999],\n",
       "   'test': [0.1736815670517328,\n",
       "    0.16082370668006027,\n",
       "    0.15539929683576093,\n",
       "    0.15278754394776495]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 1e-05, 'alpha': 100},\n",
       "  'patk': {'train': [0.66376, 0.6908, 0.7072, 0.71712],\n",
       "   'test': [0.17307885484681065,\n",
       "    0.15971873430436967,\n",
       "    0.1558011049723757,\n",
       "    0.15158211953792067]}},\n",
       " {'params': {'num_factors': 120, 'regularization': 1e-05, 'alpha': 50},\n",
       "  'patk': {'train': [0.6611199999999999, 0.6946399999999999, 0.71096, 0.7196],\n",
       "   'test': [0.17297840281265697,\n",
       "    0.1578101456554495,\n",
       "    0.15610246107483677,\n",
       "    0.15278754394776498]}},\n",
       " {'params': {'num_factors': 40, 'regularization': 1e-05, 'alpha': 100},\n",
       "  'patk': {'train': [0.6625599999999999,\n",
       "    0.6960799999999999,\n",
       "    0.7144799999999999,\n",
       "    0.7251199999999999],\n",
       "   'test': [0.17217478653942742,\n",
       "    0.16142641888498244,\n",
       "    0.15630336514314416,\n",
       "    0.1512807634354596]}},\n",
       " {'params': {'num_factors': 10, 'regularization': 0.001, 'alpha': 100},\n",
       "  'patk': {'train': [0.66496, 0.69392, 0.7092, 0.7176],\n",
       "   'test': [0.17056755399296836,\n",
       "    0.15841285786037165,\n",
       "    0.154394776494224,\n",
       "    0.1537920642893019]}}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "fig, ax = plt.subplots()\n",
    "sns.despine(fig);\n",
    "plt.plot([2, 4, 6, 8], best_curves[0]['patk']['test']);\n",
    "plt.xlabel('Epochs', fontsize=24);\n",
    "plt.ylabel('Test p@k', fontsize=24);\n",
    "plt.xticks(fontsize=18);\n",
    "plt.yticks(fontsize=18);\n",
    "plt.title('Best learning curve', fontsize=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры для поиска лучших параметров взяла меньше, так как слишком долго работает поиск\n",
    "по результатам выявлены лучшие параметры и построен график зависимости p@k от количества эпох {'num_factors': 120, 'regularization': 10.0, 'alpha': 500}\n",
    "0.23666499246609746\n",
    "Epoch: 2\n",
    "\n",
    "если анализировать результаты по всем параметрам - возможно получить лучший результат, например увеличив количество эпох"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "webinar_3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
